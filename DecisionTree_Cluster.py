# -*- coding: utf-8 -*-
"""monalisa_dokania_CSE572_Project3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wlGhlHDbVDbV-FI1sfQCaY18Gk9tyxLL
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
import seaborn as sns
from collections import Counter
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import cluster, confusion_matrix, silhouette_score, v_measure_score, adjusted_rand_score, completeness_score
from sklearn.metrics import mean_squared_error, accuracy_score
from sklearn.metrics import confusion_matrix,mean_squared_error
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.cluster import BisectingKMeans
from math import log, e, ceil
import scipy.stats as ss
from scipy.stats import entropy, iqr
from scipy import signal
from prettytable import PrettyTable
import math
import warnings
warnings.filterwarnings('ignore')
from datetime import timedelta
from scipy.fftpack import fft, ifft,rfft
from sklearn.utils import shuffle
from sklearn import metrics
# %precision 2
# %matplotlib inline

"""Read the dataset and drop NaN values :-"""

df_insulin = pd.read_csv("InsulinData.csv", usecols=["Date","Time","BWZ Carb Input (grams)"], encoding='latin', index_col=False)
cgm_data_df=pd.read_csv('CGMData.csv',low_memory=False,usecols=['Date','Time','Sensor Glucose (mg/dL)'])

df_insulin['date_time_stamp']=pd.to_datetime(df_insulin['Date'] + ' ' + df_insulin['Time'])
cgm_data_df['date_time_stamp']=pd.to_datetime(cgm_data_df['Date'] + ' ' + cgm_data_df['Time'])

df_insulin = df_insulin.dropna()
cgm_data_df = cgm_data_df.dropna()

"""# CREATE MEAL DATA :-"""

def createmealdata(insulin_data_df,cgm_data_df):
    insulin_df=insulin_data_df.copy()
    insulin_df=insulin_df.set_index('date_time_stamp')
    find_timestamp_with_2_5_hours_df=insulin_df.sort_values(by='date_time_stamp',ascending=True).dropna().reset_index()
    find_timestamp_with_2_5_hours_df['BWZ Carb Input (grams)'].replace(0.0,np.nan,inplace=True)
    find_timestamp_with_2_5_hours_df=find_timestamp_with_2_5_hours_df.dropna()
    find_timestamp_with_2_5_hours_df=find_timestamp_with_2_5_hours_df.reset_index().drop(columns='index')
    #print(find_timestamp_with_2_5_hours_df)

    valid_timestamp_list=[]
    value=0
    for idx,i in enumerate(find_timestamp_with_2_5_hours_df['date_time_stamp']):
        try:
            value=(find_timestamp_with_2_5_hours_df['date_time_stamp'][idx+1]-i).seconds / 60.0
            if value >= 120:
                valid_timestamp_list.append(i)
        except KeyError:
            break
    
    list1=[]
    for idx,i in enumerate(valid_timestamp_list):
        start=pd.to_datetime(i - timedelta(minutes=30))
        end=pd.to_datetime(i + timedelta(minutes=120))
        get_date=i.date().strftime('%-m/%-d/%Y')
        list1.append(cgm_data_df.loc[cgm_data_df['Date']==get_date].set_index('date_time_stamp').between_time(start_time=start.strftime('%-H:%-M:%-S'),end_time=end.strftime('%-H:%-M:%-S'))['Sensor Glucose (mg/dL)'].values.tolist())

    return pd.DataFrame(list1), valid_timestamp_list

meal_data, valid_timestamp_list = createmealdata(df_insulin,cgm_data_df)
meal_data=meal_data.iloc[:,0:30]
meal_data["date_time_stamp"] = valid_timestamp_list
#print(meal_data)

"""# CREATE FEATURE MATRIX :-"""

def createmealfeaturematrix(meal_data):
    index=meal_data.isna().sum(axis=1).replace(0,np.nan).dropna().where(lambda x:x>6).dropna().index
    meal_data_cleaned=meal_data.drop(meal_data.index[index]).reset_index().drop(columns='index')

    store_time = pd.DataFrame()
    store_time['date_time_stamp'] = meal_data_cleaned['date_time_stamp']
    store_time.index = meal_data_cleaned.index
    meal_data_cleaned = meal_data_cleaned.iloc[:,:-1]
    meal_data_cleaned=meal_data_cleaned.interpolate(method='linear',axis=1)
    dt_stamp=[]
    for i in range(len(meal_data_cleaned)):
      dt_stamp.append(store_time.iloc[i])

    index_to_drop_again=meal_data_cleaned.isna().sum(axis=1).replace(0,np.nan).dropna().index
    meal_data_cleaned=meal_data_cleaned.drop(meal_data.index[index_to_drop_again]).reset_index().drop(columns='index')

    meal_feature_matrix=pd.DataFrame()
    tm=6
    maximum = meal_data_cleaned.iloc[:,7:30].idxmax(axis=1)
    vel_min=[]
    vel_max=[]
    vel_avg=[]
    acl_min=[]
    acl_max=[]
    acl_avg=[]
    H=[]
    IQR=[]
    power_first_max=[]
    power_second_max=[]
    power_third_max=[]
    power_fourth_max=[]
    power_fifth_max=[]
    power_sixth_max=[]
    psd1=[]
    psd2=[]
    psd3=[]
    date_time_stamp=[]

    for i in range(len(meal_data_cleaned)):
        vel_min.append(np.diff(meal_data_cleaned.iloc[i,tm:(maximum[i]+1)].tolist()).min())
        vel_max.append(np.diff(meal_data_cleaned.iloc[i,tm:(maximum[i]+1)].tolist()).max())
        vel_avg.append(np.diff(meal_data_cleaned.iloc[i,tm:(maximum[i]+1)].tolist()).mean())
        if(len(meal_data_cleaned.iloc[i,tm:(maximum[i]+1)])>2):
            acl_min.append(np.diff(np.diff(meal_data_cleaned.iloc[i,tm:maximum[i]+1].tolist())).min())
            acl_max.append(np.diff(np.diff(meal_data_cleaned.iloc[i,tm:maximum[i]+1].tolist())).max())
            acl_avg.append(np.diff(np.diff(meal_data_cleaned.iloc[i,tm:maximum[i]+1].tolist())).mean())
        else:
            acl_min.append(0)
            acl_max.append(0)
            acl_avg.append(0)

        array=abs(rfft(meal_data_cleaned.iloc[:,0:30].iloc[i].values.tolist())).tolist()
        sorted_array=abs(rfft(meal_data_cleaned.iloc[:,0:30].iloc[i].values.tolist())).tolist()
        sorted_array.sort()
        power_first_max.append(sorted_array[-2])
        power_second_max.append(sorted_array[-3])
        power_third_max.append(sorted_array[-4])
        power_fourth_max.append(sorted_array[-5])
        power_fifth_max.append(sorted_array[-6])
        power_sixth_max.append(sorted_array[-7])   
        H.append(ss.entropy(meal_data_cleaned.iloc[i,0:30]))
        IQR.append(iqr(meal_data_cleaned.iloc[i,0:30], axis=0))   
        psd=signal.periodogram(meal_data_cleaned.iloc[:,0:30].iloc[i])
        psd1.append(psd[1][0:5].mean())  
        psd2.append(psd[1][5:10].mean()) 
        psd3.append(psd[1][10:16].mean())                                                     
    meal_feature_matrix['velocity_min']=vel_min
    meal_feature_matrix['velocity_max']=vel_max
    meal_feature_matrix['velocity_avg']=vel_avg
    meal_feature_matrix['acceleration_min']=acl_min
    meal_feature_matrix['acceleration_max']=acl_max
    meal_feature_matrix['acceleration_avg']=acl_avg
    meal_feature_matrix['entropy']=H
    meal_feature_matrix['iqr']=IQR
    meal_feature_matrix['power_first_max']=power_first_max
    meal_feature_matrix['power_second_max']=power_second_max
    meal_feature_matrix['power_third_max']=power_third_max
    meal_feature_matrix['power_fourth_max']=power_fourth_max
    meal_feature_matrix['power_fifth_max']=power_fifth_max
    meal_feature_matrix['power_sixth_max']=power_sixth_max
    meal_feature_matrix['psd1']=psd1
    meal_feature_matrix['psd2']=psd2
    meal_feature_matrix['psd3']=psd3
    meal_feature_matrix['date_time_stamp']=pd.DataFrame(dt_stamp)

    return meal_feature_matrix

meal_feature_matrix=createmealfeaturematrix(meal_data)
meal_feature_matrix1 = meal_feature_matrix
meal_feature_matrix = meal_feature_matrix.iloc[:,:-1]

print("Feature Matrix :-")
print(meal_feature_matrix)

"""# CREATE GROUND TRUTH MATRIX :-"""

def extractGroundTruth(meal_data_feature_matrix,insulin_data_df):
    df=insulin_data_df.loc[insulin_data_df["date_time_stamp"].isin(meal_data_feature_matrix["date_time_stamp"])]["BWZ Carb Input (grams)"].dropna()
    minValue = df.min()
    maxValue = df.max()
    binSize = math.ceil((maxValue-minValue)/20)
    bins=[] 
    cluster=[]
    for index in range(binSize):
        if index==0:
            bins=[minValue-1]
        else:
            bins.append(bins[index-1]+20)
        cluster.append(index)
    bins.append(maxValue)
    bin_df=pd.cut(x = df,bins = bins, labels = cluster).reset_index(drop=True)
    return meal_data_feature_matrix.iloc[:,:-1],bin_df

meal_feature_matrix, groundTruth = extractGroundTruth(meal_feature_matrix1, df_insulin)
#print(meal_feature_matrix)

# Uncomment the following lines to see the distribution of bins in the data
#sns.set(font_scale=1)
#groundTruth.iloc[:].value_counts().sort_values(ascending=True).plot(kind="barh")
#GroundTruth_matrix = pd.DataFrame()
#GroundTruth_matrix['bins'] = groundTruth.iloc[:]
print("Ground Truth or Bin matrix (Px1):-")
print(groundTruth)

"""Printing Feature Matrix -"""

print("Feature Matrix :-")
print(meal_feature_matrix)

"""Functions to calculate SSE, Entropy and Purity :-"""

def squaredError(y_true, y_pred):
    MSE = mean_squared_error(y_true, y_pred)
    counts = np.count_nonzero(y_true)
    return MSE*counts

def calc_entropy(y_actual, y_pred, base = 2):
    contingency_matrix = cluster.contingency_matrix(y_actual, y_pred)
    base = e if base is None else base
    Entropy = []
    for i in range(0, len(contingency_matrix)):
        p = contingency_matrix[i,:]
        p = pd.Series(p).value_counts(normalize=True, sort=False)
        Entropy.append((-p/p.sum() * np.log(p/p.sum())/np.log(2)).sum())
    TotalP = sum(contingency_matrix,1);
    WholeEntropy = 0;
    for i in range(0, len(contingency_matrix)):
        p = contingency_matrix[i,:]
        WholeEntropy = WholeEntropy + ((sum(p))/(sum(TotalP)))*Entropy[i]
    return WholeEntropy

def calculate_purity_score(y_true, y_pred):
    # compute contingency matrix (also called confusion matrix)
    contingency_matrix = cluster.contingency_matrix(y_true, y_pred)

    Purity = []

    for i in range(0, len(contingency_matrix)):
        p = contingency_matrix[i,:]
        Purity.append(p.max()/p.sum())

    TotalP = sum(contingency_matrix,1);
    WholePurity = 0;

    for i in range(0, len(contingency_matrix)):
        p = contingency_matrix[i,:]
        WholePurity = WholePurity + ((sum(p))/(sum(TotalP)))*Purity[i]
    
    return WholePurity

def calculate_v_measure_score(y_true, y_pred):
    return v_measure_score(y_true, y_pred)

"""# K-Means ALGORITHM :-"""

def kmeans(X, n_clusters):
    i = StandardScaler()
    X = i.fit_transform(X)
    cluster = KMeans(n_clusters = n_clusters)
    cluster.fit(X)
    print(Counter(cluster.labels_))
    sse = cluster.inertia_
    #print(cluster.inertia_)
    res = cluster.predict(X)
    return res, sse

kmeansVal, kmeans_sse  = kmeans(meal_feature_matrix,6)
cm = confusion_matrix(groundTruth, kmeansVal)
#print(kmeansVal.labels_)
print("K-means Matrix (6x6) -")
print(cm)

"""SSE, Entropy and Purity Calculation for K-Means -"""

kmean_entropy = calc_entropy(groundTruth,kmeansVal)
kmean_purity_score = calculate_purity_score(groundTruth,kmeansVal)
kmean_v_measure_score = calculate_v_measure_score(groundTruth, kmeansVal)
print("SSE for K-means = "+str(kmeans_sse))
print("Entropy for K-means = "+str(kmean_entropy))
print("Purity for K-means = "+str(kmean_purity_score))
#print(kmean_v_measure_score)

"""# DBSCAN Algorithm :-"""

def calculateDBSCAN(matrix, eps, min_samples):
    z = StandardScaler()
    X = z.fit_transform(matrix)
    val = DBSCAN(eps=eps, min_samples = min_samples).fit(X)
    #print("Labels created by DBSCAN :-")
    #print(Counter(val.labels_))
    y_pred = val.fit_predict(X)
    return y_pred, val

dbVal, db_model  = calculateDBSCAN(meal_feature_matrix, 0.82, 3)
cm1 = confusion_matrix(groundTruth, dbVal)
#print(cm1)
print("(Since the result contains outliers, there are 7 columns. But the ground truth has only 6 clusters.")
print("Hence the dimension shown is 6x7)")
print("DBSCAN Matrix :-")
cm2 = cluster.contingency_matrix(groundTruth, dbVal)
print(cm2)

"""SSE, Entropy and Purity Calculation for DBSCAN -"""

dbscan_entropy = calc_entropy(groundTruth, dbVal)
dbscan_purity_score = calculate_purity_score(groundTruth, dbVal)
dbscan_v_measure_score = calculate_v_measure_score(groundTruth, dbVal)
dbscan_sse = squaredError(groundTruth,dbVal)
print("SSE for DBSCAN = "+str(dbscan_sse))
print("Entropy for DBSCAN = "+str(dbscan_entropy))
print("Purity for DBSCAN = "+str(dbscan_purity_score))

"""# Saving Result as "Result.csv" :-"""

ldct_result = {}

ldct_result['SSE for Kmeans'] =  "%.2f"%kmeans_sse
ldct_result['SSE for DBSCAN'] =  "%.2f"%dbscan_sse
ldct_result['Entropy for Kmeans'] =  "%.2f"%kmean_entropy
ldct_result['Entropy for DBSCAN'] =  "%.2f"%dbscan_entropy
ldct_result['Purity for K means'] =  "%.2f"%kmean_purity_score
ldct_result['Purity for DBSCAN'] =  "%.2f"%dbscan_purity_score

ldf_result = pd.DataFrame(ldct_result, index=[0])
ldf_result.to_csv('Result.csv',index=False,header=False)